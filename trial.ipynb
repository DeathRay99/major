{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJPI-IXrBkrP"
   },
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: networkx in c:\\users\\deepak\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.1)\n",
      "Collecting matplotlib\n",
      "  Obtaining dependency information for matplotlib from https://files.pythonhosted.org/packages/7a/94/a1615bac6706eb8a58fe08675b80a11b5463ef9591176dd3e57257495774/matplotlib-3.8.0-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading matplotlib-3.8.0-cp310-cp310-win_amd64.whl.metadata (5.9 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Obtaining dependency information for contourpy>=1.0.1 from https://files.pythonhosted.org/packages/55/14/0dc1884e3c04f9b073a47283f5d424926644250891db392a07c56f05e5c5/contourpy-1.1.1-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading contourpy-1.1.1-cp310-cp310-win_amd64.whl.metadata (5.9 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Obtaining dependency information for fonttools>=4.22.0 from https://files.pythonhosted.org/packages/1c/c6/408ee90eae2fd7ef85c5baaedfc8d533805f4c54fc6670dbde9539f1277b/fonttools-4.42.1-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading fonttools-4.42.1-cp310-cp310-win_amd64.whl.metadata (154 kB)\n",
      "     ---------------------------------------- 0.0/154.1 kB ? eta -:--:--\n",
      "     --------------- ----------------------- 61.4/154.1 kB 1.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- 154.1/154.1 kB 1.8 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "  Obtaining dependency information for kiwisolver>=1.0.1 from https://files.pythonhosted.org/packages/4a/a1/8a9c9be45c642fa12954855d8b3a02d9fd8551165a558835a19508fec2e6/kiwisolver-1.4.5-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\deepak\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\deepak\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Collecting pillow>=6.2.0 (from matplotlib)\n",
      "  Obtaining dependency information for pillow>=6.2.0 from https://files.pythonhosted.org/packages/23/ca/7296d769f62266c0f94bf76496bc77114e7a96d2de3d7bcba91d0ba2856f/Pillow-10.0.1-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading Pillow-10.0.1-cp310-cp310-win_amd64.whl.metadata (9.6 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Obtaining dependency information for pyparsing>=2.3.1 from https://files.pythonhosted.org/packages/39/92/8486ede85fcc088f1b3dba4ce92dd29d126fd96b0008ea213167940a2475/pyparsing-3.1.1-py3-none-any.whl.metadata\n",
      "  Downloading pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\deepak\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\deepak\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.8.0-cp310-cp310-win_amd64.whl (7.6 MB)\n",
      "   ---------------------------------------- 0.0/7.6 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/7.6 MB 10.0 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.8/7.6 MB 8.2 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 1.0/7.6 MB 6.7 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.0/7.6 MB 6.4 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.0/7.6 MB 6.4 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.2/7.6 MB 4.2 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.7/7.6 MB 5.1 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.9/7.6 MB 4.9 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.0/7.6 MB 4.8 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.1/7.6 MB 4.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.1/7.6 MB 4.7 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.2/7.6 MB 4.0 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.3/7.6 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.5/7.6 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.5/7.6 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.6/7.6 MB 3.4 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.8/7.6 MB 3.5 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 2.9/7.6 MB 3.5 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.1/7.6 MB 3.4 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.2/7.6 MB 3.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 3.3/7.6 MB 3.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 3.3/7.6 MB 3.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 3.3/7.6 MB 3.1 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.6/7.6 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.7/7.6 MB 3.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 3.9/7.6 MB 3.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.1/7.6 MB 3.3 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.5/7.6 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.7/7.6 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.9/7.6 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 5.1/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 5.3/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.4/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 5.6/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.8/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.0/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.2/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.3/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.5/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.7/7.6 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.9/7.6 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.1/7.6 MB 3.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.3/7.6 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.5/7.6 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.6 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.6 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.6/7.6 MB 3.4 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.1.1-cp310-cp310-win_amd64.whl (477 kB)\n",
      "   ---------------------------------------- 0.0/478.0 kB ? eta -:--:--\n",
      "   -------------------------------- ------- 389.1/478.0 kB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 478.0/478.0 kB 5.9 MB/s eta 0:00:00\n",
      "Downloading fonttools-4.42.1-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/2.1 MB 3.6 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.2/2.1 MB 2.1 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.2/2.1 MB 2.1 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.2/2.1 MB 1.2 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.4/2.1 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.6/2.1 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.7/2.1 MB 2.3 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.8/2.1 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.0/2.1 MB 2.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.1/2.1 MB 2.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.3/2.1 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.4/2.1 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.5/2.1 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.6/2.1 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.7/2.1 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.9/2.1 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.1/2.1 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 2.6 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.5-cp310-cp310-win_amd64.whl (56 kB)\n",
      "   ---------------------------------------- 0.0/56.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 56.1/56.1 kB 3.1 MB/s eta 0:00:00\n",
      "Downloading Pillow-10.0.1-cp310-cp310-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.5 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.6/2.5 MB 8.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 1.0/2.5 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.1/2.5 MB 6.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.3/2.5 MB 6.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.5/2.5 MB 5.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.7/2.5 MB 5.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.9/2.5 MB 5.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.0/2.5 MB 5.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.2/2.5 MB 4.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.4/2.5 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 4.6 MB/s eta 0:00:00\n",
      "Downloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "   ---------------------------------------- 0.0/103.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 103.1/103.1 kB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.1.1 cycler-0.11.0 fonttools-4.42.1 kiwisolver-1.4.5 matplotlib-3.8.0 pillow-10.0.1 pyparsing-3.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install networkx\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-GyzNkI7W03D"
   },
   "outputs": [],
   "source": [
    "\n",
    "original_train_sentences = ['this is sample 1','this is sample 2']\n",
    "original_labels_train = ['postive','negative']\n",
    "original_test_sentences = ['this is sample 1','this is sample 2']\n",
    "original_labels_test = ['postive','negative']\n",
    "\n",
    "train_size = len(original_train_sentences)\n",
    "test_size = len(original_test_sentences)\n",
    "sentences = original_train_sentences + original_test_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6K9dWTv5I07_"
   },
   "source": [
    "# Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGE = 2 # 0:d2w 1:d2w+w2w 2:d2w+w2w+d2d\n",
    "NODE = 0 # 0:one-hot #1:BERT \n",
    "NUM_LAYERS = 2 \n",
    "\n",
    "HIDDEN_DIM = 200\n",
    "DROP_OUT = 0.5\n",
    "LR = 0.02\n",
    "WEIGHT_DECAY = 0\n",
    "EARLY_STOPPING = 10\n",
    "NUM_EPOCHS = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2W7wKTBfa71"
   },
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hobYcJ5OX5oT"
   },
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PtWyhXiueMOq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative' 'postive']\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "unique_labels=np.unique(original_labels_train)\n",
    "\n",
    "num_class = len(unique_labels)\n",
    "lEnc = LabelEncoder()\n",
    "lEnc.fit(unique_labels)\n",
    "\n",
    "print(unique_labels)\n",
    "print(lEnc.transform(unique_labels))\n",
    "\n",
    "train_labels = lEnc.transform(original_labels_train)\n",
    "test_labels = lEnc.transform(original_labels_test)\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "labels = train_labels.tolist()+test_labels.tolist()\n",
    "labels = torch.LongTensor(labels).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMkEBxr6fMQi"
   },
   "source": [
    "## Remove Stopwords and less frequent words, tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1xRG94uDfaBV"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "remove_limit = 5\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "original_word_freq = {}  # to remove rare words\n",
    "for sentence in sentences:\n",
    "    temp = clean_str(sentence)\n",
    "    word_list = temp.split()\n",
    "    for word in word_list:\n",
    "        if word in original_word_freq:\n",
    "            original_word_freq[word] += 1\n",
    "        else:\n",
    "            original_word_freq[word] = 1   \n",
    "\n",
    "tokenize_sentences = []\n",
    "word_list_dict = {}\n",
    "for sentence in sentences:\n",
    "    temp = clean_str(sentence)\n",
    "    word_list_temp = temp.split()\n",
    "    doc_words = []\n",
    "    for word in word_list_temp: \n",
    "        if word in original_word_freq and word not in stop_words and original_word_freq[word] >= remove_limit:\n",
    "            doc_words.append(word)\n",
    "            word_list_dict[word] = 1\n",
    "    tokenize_sentences.append(doc_words)\n",
    "word_list = list(word_list_dict.keys())\n",
    "vocab_length = len(word_list)\n",
    "\n",
    "#word to id dict\n",
    "word_id_map = {}\n",
    "for i in range(vocab_length):\n",
    "    word_id_map[word_list[i]] = i            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dqLUncB2Pn_L"
   },
   "outputs": [],
   "source": [
    "node_size = train_size + vocab_length + test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0o8wcXgrTiD"
   },
   "source": [
    "# Model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EZbRV2wYxY1U"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znJ7Grz7fQ2L"
   },
   "source": [
    "## Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-BSg1uNgV3_7"
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "row = []\n",
    "col = []\n",
    "weight = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QESQPT88AqsI"
   },
   "source": [
    "### word-word: PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KNlJoLFagXhv"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce1ba40d02d43b7a073e5157f477221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if EDGE >= 1:\n",
    "    window_size = 20\n",
    "    total_W = 0\n",
    "    word_occurrence = {}\n",
    "    word_pair_occurrence = {}\n",
    "\n",
    "    def ordered_word_pair(a, b):\n",
    "        if a > b:\n",
    "            return b, a\n",
    "        else:\n",
    "            return a, b\n",
    "\n",
    "    def update_word_and_word_pair_occurrence(q):\n",
    "        unique_q = list(set(q))\n",
    "        for i in unique_q:\n",
    "            try:\n",
    "                word_occurrence[i] += 1\n",
    "            except:\n",
    "                word_occurrence[i] = 1\n",
    "        for i in range(len(unique_q)):\n",
    "            for j in range(i+1, len(unique_q)):\n",
    "                word1 = unique_q[i]\n",
    "                word2 = unique_q[j]\n",
    "                word1, word2 = ordered_word_pair(word1, word2)\n",
    "                try:\n",
    "                    word_pair_occurrence[(word1, word2)] += 1\n",
    "                except:\n",
    "                    word_pair_occurrence[(word1, word2)] = 1\n",
    "\n",
    "\n",
    "    for ind in tqdm(range(train_size+test_size)):\n",
    "        words = tokenize_sentences[ind]\n",
    "\n",
    "        q = []\n",
    "        # push the first (window_size) words into a queue\n",
    "        for i in range(min(window_size, len(words))):\n",
    "            q += [word_id_map[words[i]]]\n",
    "        # update the total number of the sliding windows\n",
    "        total_W += 1\n",
    "        # update the number of sliding windows that contain each word and word pair\n",
    "        update_word_and_word_pair_occurrence(q)\n",
    "\n",
    "        now_next_word_index = window_size\n",
    "        # pop the first word out and let the next word in, keep doing this until the end of the document\n",
    "        while now_next_word_index<len(words):\n",
    "            q.pop(0)\n",
    "            q += [word_id_map[words[now_next_word_index]]]\n",
    "            now_next_word_index += 1\n",
    "            # update the total number of the sliding windows\n",
    "            total_W += 1\n",
    "            # update the number of sliding windows that contain each word and word pair\n",
    "            update_word_and_word_pair_occurrence(q)\n",
    "\n",
    "    for word_pair in word_pair_occurrence:\n",
    "        i = word_pair[0]\n",
    "        j = word_pair[1]\n",
    "        count = word_pair_occurrence[word_pair]\n",
    "        word_freq_i = word_occurrence[i]\n",
    "        word_freq_j = word_occurrence[j]\n",
    "        pmi = log((count * total_W) / (word_freq_i * word_freq_j))\n",
    "        if pmi <=0:\n",
    "            continue\n",
    "        row.append(train_size + i)\n",
    "        col.append(train_size + j)\n",
    "        weight.append(pmi)\n",
    "        row.append(train_size + j)\n",
    "        col.append(train_size + i)\n",
    "        weight.append(pmi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hynLnT3a33kW"
   },
   "source": [
    "### doc-word: Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BnSPqhg1lHps"
   },
   "outputs": [],
   "source": [
    "#get each word appears in which document\n",
    "word_doc_list = {}\n",
    "for word in word_list:\n",
    "    word_doc_list[word]=[]\n",
    "\n",
    "for i in range(len(tokenize_sentences)):\n",
    "    doc_words = tokenize_sentences[i]\n",
    "    unique_words = set(doc_words)\n",
    "    for word in unique_words:\n",
    "        exsit_list = word_doc_list[word]\n",
    "        exsit_list.append(i)\n",
    "        word_doc_list[word] = exsit_list\n",
    "\n",
    "#document frequency\n",
    "word_doc_freq = {}\n",
    "for word, doc_list in word_doc_list.items():\n",
    "    word_doc_freq[word] = len(doc_list)\n",
    "\n",
    "# term frequency\n",
    "doc_word_freq = {}\n",
    "\n",
    "for doc_id in range(len(tokenize_sentences)):\n",
    "    words = tokenize_sentences[doc_id]\n",
    "    for word in words:\n",
    "        word_id = word_id_map[word]\n",
    "        doc_word_str = str(doc_id) + ',' + str(word_id)\n",
    "        if doc_word_str in doc_word_freq:\n",
    "            doc_word_freq[doc_word_str] += 1\n",
    "        else:\n",
    "            doc_word_freq[doc_word_str] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Z6elPPFO_sXp"
   },
   "outputs": [],
   "source": [
    "for i in range(len(tokenize_sentences)):\n",
    "    words = tokenize_sentences[i]\n",
    "    doc_word_set = set()\n",
    "    for word in words:\n",
    "        if word in doc_word_set:\n",
    "            continue\n",
    "        j = word_id_map[word]\n",
    "        key = str(i) + ',' + str(j)\n",
    "        freq = doc_word_freq[key]\n",
    "        if i < train_size:\n",
    "            row.append(i)\n",
    "        else:\n",
    "            row.append(i + vocab_length)\n",
    "        col.append(train_size + j)\n",
    "        idf = log(1.0 * len(tokenize_sentences) / word_doc_freq[word_list[j]])\n",
    "        weight.append(freq * idf)\n",
    "        doc_word_set.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAr6ygKhWTc-"
   },
   "source": [
    "### doc-doc: jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "T4-EH15oWWSX"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnx\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m EDGE \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m      6\u001b[0m     G \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mGraph()  \u001b[38;5;66;03m# Create an empty graph\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if EDGE >= 2:\n",
    "    G = nx.Graph()  # Create an empty graph\n",
    "\n",
    "    tokenize_sentences_set = [set(s) for s in tokenize_sentences]\n",
    "    jaccard_threshold = 0.2\n",
    "\n",
    "    for i in tqdm(range(len(tokenize_sentences))):\n",
    "        G.add_node(i)  # Add a node for each document\n",
    "        for j in range(i + 1, len(tokenize_sentences)):\n",
    "            union_len = len(tokenize_sentences_set[i].union(tokenize_sentences_set[j]))\n",
    "            if union_len != 0:\n",
    "                jaccard_w = 1 - nltk.jaccard_distance(tokenize_sentences_set[i], tokenize_sentences_set[j])\n",
    "                if jaccard_w > jaccard_threshold:\n",
    "                    G.add_edge(i, j, weight=jaccard_w)  # Add an edge between documents\n",
    "\n",
    "    # Save the graph to a file\n",
    "    nx.write_gexf(G, 'graph.gexf')\n",
    "\n",
    "    # Visualize the graph\n",
    "    pos = nx.spring_layout(G)\n",
    "    labels = {i: i for i in range(len(tokenize_sentences))}\n",
    "    nx.draw(G, pos, labels=labels, node_color='skyblue', font_size=8, font_color='black', node_size=200)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIkGgB2aZDk7"
   },
   "source": [
    "### Adjacent matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "C0O1Ucdhod9a"
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "adj = sp.csr_matrix((weight, (row, col)), shape=(node_size, node_size))\n",
    "\n",
    "# build symmetric adjacency matrix\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ivyuexATkQFW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepa\\AppData\\Local\\Temp\\ipykernel_144592\\3533750465.py:20: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:607.)\n",
      "  return torch.sparse.FloatTensor(indices, values, shape).to(device)\n"
     ]
    }
   ],
   "source": [
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo(), d_inv_sqrt\n",
    "    \n",
    "adj, norm_item = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape).to(device)\n",
    "\n",
    "adj = sparse_mx_to_torch_sparse_tensor(adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMgbhTstMSUA"
   },
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "mP9dqCskOrXT"
   },
   "outputs": [],
   "source": [
    "if NODE == 0:\n",
    "    features = np.arange(node_size)\n",
    "    features = torch.FloatTensor(features).to(device)\n",
    "else:\n",
    "    \n",
    "    from flair.embeddings import TransformerDocumentEmbeddings, TransformerWordEmbeddings\n",
    "    from flair.data import Sentence\n",
    "    doc_embedding = TransformerDocumentEmbeddings('bert-base-uncased', fine_tune=False)\n",
    "    word_embedding = TransformerWordEmbeddings('bert-base-uncased', layers='-1',subtoken_pooling=\"mean\")\n",
    "\n",
    "    sent_embs = []\n",
    "    word_embs = {}\n",
    "\n",
    "    for ind in tqdm(range(train_size+test_size)):\n",
    "        sent = tokenize_sentences[ind]\n",
    "        sentence = Sentence(\" \".join(sent[:512]),use_tokenizer=False)\n",
    "        doc_embedding.embed(sentence)\n",
    "        sent_embs.append(sentence.get_embedding().tolist())\n",
    "        words = Sentence(\" \".join(sent[:512]),use_tokenizer=False)\n",
    "        word_embedding.embed(words)\n",
    "        for token in words:\n",
    "            word = token.text\n",
    "            embedding = token.embedding.tolist()\n",
    "            if word not in word_embs:\n",
    "                word_embs[word] = embedding\n",
    "            else:\n",
    "                word_embs[word] = np.minimum(word_embs[word], embedding)\n",
    "\n",
    "    word_embs_list = []\n",
    "    for word in word_list:\n",
    "        word_embs_list.append(word_embs[word])\n",
    "\n",
    "    features = sent_embs[:train_size] + word_embs_list + sent_embs[train_size:]\n",
    "\n",
    "    import scipy.sparse as sp\n",
    "    def preprocess_features(features):\n",
    "        \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "        rowsum = np.array(features.sum(1))\n",
    "        r_inv = np.power(rowsum, -1).flatten()\n",
    "        r_inv[np.isinf(r_inv)] = 0.\n",
    "        r_mat_inv = sp.diags(r_inv)\n",
    "        features = r_mat_inv.dot(features)\n",
    "        return features\n",
    "\n",
    "    features = preprocess_features(sp.csr_matrix(features)).todense()\n",
    "    features = torch.FloatTensor(features).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdx6RrUvjbF0"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39Kj8NQujiDH"
   },
   "source": [
    "## GCN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "jNVkA-h7b3sP"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features,  drop_out = 0, activation=None, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.zeros(1, out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters(in_features, out_features)\n",
    "        self.dropout = torch.nn.Dropout(drop_out)\n",
    "        self.activation =  activation\n",
    "\n",
    "    def reset_parameters(self,in_features, out_features):\n",
    "        stdv = np.sqrt(6.0/(in_features+out_features))\n",
    "        # stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        # if self.bias is not None:\n",
    "        #     torch.nn.init.zeros_(self.bias)\n",
    "            # self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "\n",
    "    def forward(self, input, adj, feature_less = False):\n",
    "        if feature_less:\n",
    "            support = self.weight\n",
    "            support = self.dropout(support)\n",
    "        else:\n",
    "            input = self.dropout(input)\n",
    "            support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "k57M4sz4s4Md"
   },
   "source": [
    "## GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "aJ-ZQuMzs5tZ"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, n_layers = 2):\n",
    "        super(GCN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.gc_list = []\n",
    "        if n_layers >= 2:\n",
    "            self.gc1 = GraphConvolution(nfeat, nhid, dropout, activation = nn.ReLU())\n",
    "            self.gc_list = nn.ModuleList([GraphConvolution(nhid, nhid, dropout, activation = nn.ReLU()) for _ in range(self.n_layers-2)])\n",
    "            self.gcf = GraphConvolution(nhid, nclass, dropout)\n",
    "        else:\n",
    "            self.gc1 = GraphConvolution(nfeat, nclass, dropout)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        if self.n_layers>=2:\n",
    "            x = self.gc1(x, adj, feature_less = True)\n",
    "            for i in range(self.n_layers-2):\n",
    "                x = self.gc_list[i](x,adj)\n",
    "            x = self.gcf(x,adj)\n",
    "        else:\n",
    "            x = self.gc1(x, adj, feature_less = True)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "qmhOG1yG--Ji"
   },
   "outputs": [],
   "source": [
    "def cal_accuracy(predictions,labels):\n",
    "    pred = torch.argmax(predictions,-1).cpu().tolist()\n",
    "    lab = labels.cpu().tolist()\n",
    "    cor = 0\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == lab[i]:\n",
    "            cor += 1\n",
    "    return cor/len(pred)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "zEE4JxeUthCb"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "bIxII4QoticA"
   },
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "hdNsgxMG-Wwu"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = GCN(nfeat=node_size, nhid=HIDDEN_DIM, nclass=num_class, dropout=DROP_OUT,n_layers=NUM_LAYERS).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "T98r4qZuuFyn"
   },
   "source": [
    "## Training and Validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Bv9br9pgGw9R"
   },
   "outputs": [],
   "source": [
    "def generate_train_val(train_pro=0.9):\n",
    "    real_train_size = int(train_pro*train_size)\n",
    "    val_size = train_size-real_train_size\n",
    "\n",
    "    idx_train = np.random.choice(train_size, real_train_size,replace=False)\n",
    "    idx_train.sort()\n",
    "    idx_val = []\n",
    "    pointer = 0\n",
    "    for v in range(train_size):\n",
    "        if pointer<len(idx_train) and idx_train[pointer] == v:\n",
    "            pointer +=1\n",
    "        else:\n",
    "            idx_val.append(v)\n",
    "    idx_test = range(train_size+vocab_length, node_size)\n",
    "    return idx_train, idx_val, idx_test\n",
    "\n",
    "idx_train, idx_val, idx_test = generate_train_val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "QC7u3Jn2uIu4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 0.8835 acc_train: 0.0000 loss_val: 0.8123 acc_val: 0.0000 time: 0.1261s\n",
      "Epoch: 0002 loss_train: 0.6037 acc_train: 1.0000 loss_val: 0.8795 acc_val: 0.0000 time: 0.0061s\n",
      "Epoch: 0003 loss_train: 0.6847 acc_train: 1.0000 loss_val: 0.9509 acc_val: 0.0000 time: 0.0037s\n",
      "Epoch: 0004 loss_train: 0.5333 acc_train: 1.0000 loss_val: 1.0404 acc_val: 0.0000 time: 0.0037s\n",
      "Epoch: 0005 loss_train: 0.2572 acc_train: 1.0000 loss_val: 1.1486 acc_val: 0.0000 time: 0.0020s\n",
      "Epoch: 0006 loss_train: 0.1312 acc_train: 1.0000 loss_val: 1.2725 acc_val: 0.0000 time: 0.0058s\n",
      "Epoch: 0007 loss_train: 0.3748 acc_train: 1.0000 loss_val: 1.4205 acc_val: 0.0000 time: 0.0040s\n",
      "Epoch: 0008 loss_train: 0.0914 acc_train: 1.0000 loss_val: 1.5831 acc_val: 0.0000 time: 0.0054s\n",
      "Epoch: 0009 loss_train: 0.0633 acc_train: 1.0000 loss_val: 1.7619 acc_val: 0.0000 time: 0.0052s\n",
      "Epoch: 0010 loss_train: 0.1618 acc_train: 1.0000 loss_val: 1.9678 acc_val: 0.0000 time: 0.0040s\n",
      "Epoch: 0011 loss_train: 0.0497 acc_train: 1.0000 loss_val: 2.1885 acc_val: 0.0000 time: 0.0040s\n",
      "Epoch: 0012 loss_train: 0.0269 acc_train: 1.0000 loss_val: 2.4144 acc_val: 0.0000 time: 0.0035s\n",
      "Early Stopping...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def train_model(show_result = True):\n",
    "    val_loss = []\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        t = time.time()\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output= model(features, adj)\n",
    "        loss_train = criterion(output[idx_train], labels[idx_train])\n",
    "        acc_train = cal_accuracy(output[idx_train], labels[idx_train])\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "        loss_val = criterion(output[idx_val], labels[idx_val])\n",
    "        val_loss.append(loss_val.item())\n",
    "        acc_val = cal_accuracy(output[idx_val], labels[idx_val])\n",
    "        if show_result:\n",
    "            print(  'Epoch: {:04d}'.format(epoch+1),\n",
    "                    'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                    'acc_train: {:.4f}'.format(acc_train),\n",
    "                    'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                    'acc_val: {:.4f}'.format(acc_val),\n",
    "                    'time: {:.4f}s'.format(time.time() - t))\n",
    "        \n",
    "        if epoch > EARLY_STOPPING and np.min(val_loss[-EARLY_STOPPING:]) > np.min(val_loss[:-EARLY_STOPPING]) :\n",
    "            if show_result:\n",
    "                print(\"Early Stopping...\")\n",
    "            break\n",
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "OQwlWq6dyYJm"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "jmPNukmk40gd"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [6, 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Call the test function and pass the actual labels\u001b[39;00m\n\u001b[0;32m     18\u001b[0m test_labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m---> 19\u001b[0m acc, f11, f12 \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, F1-score (macro): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf11\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, F1-score (weighted): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf12\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[31], line 7\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(test_labels)\u001b[0m\n\u001b[0;32m      5\u001b[0m output \u001b[38;5;241m=\u001b[39m model(features, adj)\n\u001b[0;32m      6\u001b[0m predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(output[idx_test], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m----> 7\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m f11 \u001b[38;5;241m=\u001b[39m f1_score(test_labels, predictions, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m f12 \u001b[38;5;241m=\u001b[39m f1_score(test_labels, predictions, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    212\u001b[0m         )\n\u001b[0;32m    213\u001b[0m     ):\n\u001b[1;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:220\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:84\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    405\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    410\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [6, 2]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    predictions = torch.argmax(output[idx_test],-1).cpu().tolist()\n",
    "    acc = accuracy_score(test_labels,predictions)\n",
    "    f11 = f1_score(test_labels,predictions, average='macro')\n",
    "    f12 = f1_score(test_labels,predictions, average = 'weighted')\n",
    "    return acc, f11, f12\n",
    "\n",
    "print(test())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "LOFsVlv4hTgc"
   },
   "source": [
    "# Test 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ydMqrCkehVPW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n",
      "Macro F1: 0.3333\n",
      "Weighted F1: 0.3333\n"
     ]
    }
   ],
   "source": [
    "test_acc_list = []\n",
    "test_f11_list = []\n",
    "test_f12_list = []\n",
    "\n",
    "for t in range(10):\n",
    "    model = GCN(nfeat=node_size, nhid=HIDDEN_DIM, nclass=num_class, dropout=DROP_OUT,n_layers=NUM_LAYERS).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    idx_train, idx_val, idx_test = generate_train_val()\n",
    "    train_model(show_result=False)\n",
    "    acc, f11, f12 = test()\n",
    "    test_acc_list.append(acc)\n",
    "    test_f11_list.append(f11)\n",
    "    test_f12_list.append(f12)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\",np.round(np.mean(test_acc_list),4))\n",
    "print(\"Macro F1:\",np.round(np.mean(test_f11_list),4))\n",
    "print(\"Weighted F1:\",np.round(np.mean(test_f12_list),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "final code.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
